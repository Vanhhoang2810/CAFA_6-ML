{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Imports\n\nimport os, glob, gzip, re\nimport pandas as pd, numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom scipy import sparse\nimport math\n\n!pip install -q biopython\n\n# test import immediately\ntry:\n    from Bio import SeqIO\n    print('Biopython installed and import OK:', SeqIO.__name__)\nexcept Exception as e:\n    print('Import failed — you may need to restart the kernel. Error:', e) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:15.709426Z","iopub.execute_input":"2025-12-13T10:36:15.709814Z","iopub.status.idle":"2025-12-13T10:36:18.854636Z","shell.execute_reply.started":"2025-12-13T10:36:15.709782Z","shell.execute_reply":"2025-12-13T10:36:18.853610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#File Detection\n\nINPUT_ROOT = '/kaggle/input'\nall_files = glob.glob(os.path.join(INPUT_ROOT, '**', '*'), recursive=True)\n\nfasta_train = None\nfasta_test = None\nterms_tsv = None\n\nfor p in all_files:\n    low = p.lower()\n    if low.endswith('.fasta') or low.endswith('.fa') or low.endswith('.fasta.gz') or low.endswith('.fa.gz'):\n        if 'train' in low and fasta_train is None:\n            fasta_train = p\n        elif 'test' in low and fasta_test is None:\n            fasta_test = p\n        elif fasta_train is None:\n            fasta_train = p\n    if (low.endswith('.tsv') or low.endswith('.csv')) and terms_tsv is None:\n        if 'term' in low or 'label' in low or 'annotation' in low or 'train_terms' in low:\n            terms_tsv = p\n\ncomp_folder = os.path.join(INPUT_ROOT, 'cafa-6-protein-function-prediction')\nif os.path.exists(comp_folder):\n    if fasta_train is None and os.path.exists(os.path.join(comp_folder, 'train_sequences.fasta')):\n        fasta_train = os.path.join(comp_folder, 'train_sequences.fasta')\n    if fasta_test is None and os.path.exists(os.path.join(comp_folder, 'test_sequences.fasta')):\n        fasta_test = os.path.join(comp_folder, 'test_sequences.fasta')\n    if terms_tsv is None and os.path.exists(os.path.join(comp_folder, 'train_terms.tsv')):\n        terms_tsv = os.path.join(comp_folder, 'train_terms.tsv')\n\nif not fasta_train or not fasta_test or not terms_tsv:\n    print('Auto-detection failed. Found:')\n    print(' fasta_train =', fasta_train)\n    print(' fasta_test  =', fasta_test)\n    print(' terms_tsv   =', terms_tsv)\n    raise SystemExit('Attach the CAFA-6 dataset: Add data -> cafa-6-protein-function-prediction')\n\nprint('Using files:')\nprint(' train fasta:', fasta_train)\nprint(' test  fasta:', fasta_test)\nprint(' terms tsv :', terms_tsv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.856169Z","iopub.execute_input":"2025-12-13T10:36:18.856378Z","iopub.status.idle":"2025-12-13T10:36:18.869456Z","shell.execute_reply.started":"2025-12-13T10:36:18.856358Z","shell.execute_reply":"2025-12-13T10:36:18.868916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Defining Helpers\n\ndef read_fasta_ids(path, limit=None):\n    open_fn = gzip.open if str(path).endswith('.gz') else open\n    ids = []\n    with open_fn(path, 'rt') as handle:\n        for rec in SeqIO.parse(handle, 'fasta'):\n            ids.append(rec.id)\n            if limit and len(ids) >= limit:\n                break\n    return ids\n\ndef read_fasta_dict(path):\n    open_fn = gzip.open if str(path).endswith('.gz') else open\n    seqs = {}\n    with open_fn(path, 'rt') as handle:\n        for rec in SeqIO.parse(handle, 'fasta'):\n            seqs[rec.id] = str(rec.seq)\n    return seqs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.870078Z","iopub.execute_input":"2025-12-13T10:36:18.870299Z","iopub.status.idle":"2025-12-13T10:36:18.878004Z","shell.execute_reply.started":"2025-12-13T10:36:18.870283Z","shell.execute_reply":"2025-12-13T10:36:18.877286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#previewing and debudding\n\ntrain_sample_ids = read_fasta_ids(fasta_train, limit=20)\ntest_sample_ids = read_fasta_ids(fasta_test, limit=20)\nprint('\\nExample train FASTA ids (20):', train_sample_ids[:20])\nprint('Example test  FASTA ids (20):', test_sample_ids[:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.879125Z","iopub.execute_input":"2025-12-13T10:36:18.879351Z","iopub.status.idle":"2025-12-13T10:36:18.897002Z","shell.execute_reply.started":"2025-12-13T10:36:18.879336Z","shell.execute_reply":"2025-12-13T10:36:18.896265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#read and detect terms_tsv format\n\nopen_fn = gzip.open if str(terms_tsv).endswith('.gz') else open\nprint('\\n--- first 12 raw lines of terms_tsv ---')\nwith open_fn(terms_tsv, 'rt') as f:\n    for i, line in enumerate(f):\n        print(i+1, line.strip())\n        if i >= 11:\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.898899Z","iopub.execute_input":"2025-12-13T10:36:18.899122Z","iopub.status.idle":"2025-12-13T10:36:18.908362Z","shell.execute_reply.started":"2025-12-13T10:36:18.899107Z","shell.execute_reply":"2025-12-13T10:36:18.907709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#trying with more rows \n\ndf = pd.read_csv(terms_tsv, sep='\\t', header=None, dtype=str, engine='python')\nprint('\\nRead shape (terms file):', df.shape)\nprint('First rows:')\nprint(df.head(6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.909230Z","iopub.execute_input":"2025-12-13T10:36:18.909604Z","iopub.status.idle":"2025-12-13T10:36:20.493105Z","shell.execute_reply.started":"2025-12-13T10:36:18.909581Z","shell.execute_reply":"2025-12-13T10:36:20.492227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# heuristic- check for GO or similar\n\ncol0_is_go = df[0].astype(str).str.match(r'^GO:\\\\d{7}').sum() if 0 in df.columns else 0\ncol1_is_go = df[1].astype(str).str.match(r'^GO:\\\\d{7}').sum() if 1 in df.columns else 0\nprint(f'col0 GO-like count = {col0_is_go}, col1 GO-like count = {col1_is_go}')\n\nif col0_is_go > col1_is_go:\n    print('Detected first column contains GO terms. Interpreting file as (term_id, protein_id) -> swapping.')\n    terms_df = df.rename(columns={0:'term_id', 1:'protein_id'})[['protein_id','term_id']]\nelse:\n    print('Detected first column contains protein ids. Using (protein_id, term_id).')\n    terms_df = df.rename(columns={0:'protein_id', 1:'term_id'})[['protein_id','term_id']]\n\nprint('\\nterms_df sample:')\nprint(terms_df.head(6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:20.494136Z","iopub.execute_input":"2025-12-13T10:36:20.494533Z","iopub.status.idle":"2025-12-13T10:36:20.927443Z","shell.execute_reply.started":"2025-12-13T10:36:20.494486Z","shell.execute_reply":"2025-12-13T10:36:20.926719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Normalization \n\ndef norm_variants(s):\n    s = '' if s is None else str(s)\n    out = []\n    out.append(s)\n    if '|' in s:\n        parts = s.split('|')\n        for p in parts:\n            if p: out.append(p)\n        out.append(parts[-1])\n        if len(parts) > 1: out.append(parts[1])\n    out.append(s.split()[0])\n    if '.' in s: out.append(s.split('.')[0])\n    out.append(re.sub('[^A-Za-z0-9_\\\\-]', '', s))\n    for t in re.split('[\\\\|\\\\s]', s):\n        t = t.strip()\n        if 4 <= len(t) <= 12:\n            out.append(t)\n    uniq = []\n    for v in out:\n        if v and v not in uniq: uniq.append(v)\n    return uniq\n\nlabel_ids = pd.unique(terms_df['protein_id'])\nnorm2labels = {}\nfor lid in label_ids:\n    for nv in norm_variants(lid):\n        norm2labels.setdefault(nv, set()).add(lid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:20.928336Z","iopub.execute_input":"2025-12-13T10:36:20.928604Z","iopub.status.idle":"2025-12-13T10:36:21.193943Z","shell.execute_reply.started":"2025-12-13T10:36:20.928586Z","shell.execute_reply":"2025-12-13T10:36:21.193369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('\\nSample match count (train sample):', sum(1 for tid in train_sample_ids if any(c in norm2labels for c in norm_variants(tid))), '/', len(train_sample_ids))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:21.194661Z","iopub.execute_input":"2025-12-13T10:36:21.194869Z","iopub.status.idle":"2025-12-13T10:36:21.199366Z","shell.execute_reply.started":"2025-12-13T10:36:21.194852Z","shell.execute_reply":"2025-12-13T10:36:21.198772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Map train (fasta ids -> lable ids)\n\ntrain_all_ids = read_fasta_ids(fasta_train, limit=None)\nmapped_train_to_label = {}\nunmatched = []\nfor tid in train_all_ids:\n    found = False\n    for cand in norm_variants(tid):\n        if cand in norm2labels:\n            mapped_train_to_label[tid] = list(norm2labels[cand])[0]\n            found = True\n            break\n    if not found:\n        unmatched.append(tid)\n\nprint('Total train sequences:', len(train_all_ids))\nprint('Mapped train->label count:', len(mapped_train_to_label))\nprint('Unmatched train sequences:', len(unmatched))\nprint('Example unmatched (10):', unmatched[:10])\n\nif len(mapped_train_to_label) == 0:\n    raise SystemExit('No mapping between fasta ids and label ids found automatically. Paste the first raw lines above here for further rule craft.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:21.200147Z","iopub.execute_input":"2025-12-13T10:36:21.200443Z","iopub.status.idle":"2025-12-13T10:36:21.963063Z","shell.execute_reply.started":"2025-12-13T10:36:21.200427Z","shell.execute_reply":"2025-12-13T10:36:21.962436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#mapping terms_df\n\nlabel_to_fasta = {}\nfor f_id, l_id in mapped_train_to_label.items():\n    label_to_fasta.setdefault(l_id, set()).add(f_id)\n\ndef map_label_row(lid):\n    if lid in label_to_fasta:\n        return list(label_to_fasta[lid])[0]\n    for nv in norm_variants(lid):\n        if nv in label_to_fasta:\n            return list(label_to_fasta[nv])[0]\n    return None\n\nterms_df['mapped_fasta_id'] = terms_df['protein_id'].apply(map_label_row)\nmapped_count = int(terms_df['mapped_fasta_id'].notnull().sum())\nprint('Mapped label rows to fasta ids:', mapped_count, '/', len(terms_df))\n\nmapped_df = terms_df[terms_df['mapped_fasta_id'].notnull()].copy()\ngrouped = mapped_df.groupby('mapped_fasta_id')['term_id'].apply(list).reset_index().rename(columns={'mapped_fasta_id':'protein_id'})\nprint('Grouped unique proteins with labels (after mapping):', len(grouped))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:21.963783Z","iopub.execute_input":"2025-12-13T10:36:21.964101Z","iopub.status.idle":"2025-12-13T10:36:24.031870Z","shell.execute_reply.started":"2025-12-13T10:36:21.964078Z","shell.execute_reply":"2025-12-13T10:36:24.031096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading seq dict\n\ntrain_seqs = read_fasta_dict(fasta_train)\ntest_seqs = read_fasta_dict(fasta_test)\n\ngrouped = grouped[grouped['protein_id'].isin(train_seqs.keys())].reset_index(drop=True)\nprint('Grouped after filtering to available sequences:', len(grouped))\nif len(grouped) == 0:\n    raise SystemExit('No labelled proteins remain after mapping and filtering. Inspect unmatched examples above.')\n\ngrouped['sequence'] = grouped['protein_id'].map(train_seqs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:24.032776Z","iopub.execute_input":"2025-12-13T10:36:24.033110Z","iopub.status.idle":"2025-12-13T10:36:25.434303Z","shell.execute_reply.started":"2025-12-13T10:36:24.033088Z","shell.execute_reply":"2025-12-13T10:36:25.433532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#k-mers\n\nK = 3\ndef kmers(seq, k=K):\n    if not isinstance(seq, str) or len(seq) == 0: return ['']\n    if len(seq) < k: return [seq]\n    return [seq[i:i+k] for i in range(len(seq)-k+1)]\n\ntrain_texts = [' '.join(kmers(s)) for s in grouped['sequence']]\n\nvectorizer = HashingVectorizer(n_features=2**15, alternate_sign=False, token_pattern=r'[^\\\\s]+')  # 32k features\nX_full = vectorizer.transform(train_texts)\nif 'mlb' not in globals():\n    from sklearn.preprocessing import MultiLabelBinarizer\n    mlb = MultiLabelBinarizer(sparse_output=True)\nY_sparse = mlb.fit_transform(grouped['term_id'])\nprint('X_full shape:', X_full.shape, 'Y shape:', Y_sparse.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:25.435210Z","iopub.execute_input":"2025-12-13T10:36:25.435426Z","iopub.status.idle":"2025-12-13T10:36:35.745082Z","shell.execute_reply.started":"2025-12-13T10:36:25.435402Z","shell.execute_reply":"2025-12-13T10:36:35.744261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training and saving artifact\nimport numpy as np\nimport pandas as pd\nimport gc\nimport pickle\nfrom scipy import sparse\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nBASE_PATH = '/kaggle/input/cafa-6-protein-function-prediction'\nMODEL_DIR = '/kaggle/working' \nNGRAM_RANGE = (3, 4)\nMAX_FEATURES = 20000\n\nprint(\"--- BẮT ĐẦU: TRAINING ---\")\n\nprint(\"1. Đang tải dữ liệu Train...\")\n# Hàm đọc Fasta nhanh\ndef read_fasta(path):\n    seqs = {}\n    with open(path, 'r') as f:\n        cid, cseq = \"\", []\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if cid: seqs[cid] = \"\".join(cseq)\n                parts = line.split('|')\n                cid = parts[1] if len(parts) > 1 else line[1:].split()[0]\n                cseq = []\n            else:\n                cseq.append(line)\n        if cid: seqs[cid] = \"\".join(cseq)\n    return seqs\n\ntrain_seqs = read_fasta(f\"{BASE_PATH}/Train/train_sequences.fasta\")\ndf_terms = pd.read_csv(f\"{BASE_PATH}/Train/train_terms.tsv\", sep='\\t')\n\nprint(\"   -> Map nhãn vào sequence...\")\ngrouped = df_terms.groupby('EntryID')['term'].apply(list).reset_index()\ngrouped_map = dict(zip(grouped['EntryID'], grouped['term']))\n\ntrain_ids = list(train_seqs.keys())\ntrain_sentences = [train_seqs[pid] for pid in train_ids]\ntrain_labels = [grouped_map.get(pid, []) for pid in train_ids]\n\nmask = [len(x) > 0 for x in train_labels]\ntrain_sentences = [x for i, x in enumerate(train_sentences) if mask[i]]\ntrain_labels = [x for i, x in enumerate(train_labels) if mask[i]]\n\nprint(f\"   -> Số lượng mẫu Train hợp lệ: {len(train_sentences)}\")\ndel train_seqs, df_terms, grouped\ngc.collect()\n\nprint(\"2. Training Vectorizer (TF-IDF)...\")\nvectorizer = TfidfVectorizer(\n    analyzer='char', ngram_range=NGRAM_RANGE, \n    max_features=MAX_FEATURES, dtype=np.float32, sublinear_tf=True\n)\nX_train = vectorizer.fit_transform(train_sentences)\n\nprint(\"3. Binarizing Labels...\")\nmlb = MultiLabelBinarizer(sparse_output=True)\nY_train = mlb.fit_transform(train_labels)\n\nprint(f\"4. Lưu model xuống {MODEL_DIR}...\")\n\nwith open(f'{MODEL_DIR}/vectorizer.pkl', 'wb') as f:\n    pickle.dump(vectorizer, f)\n    \nwith open(f'{MODEL_DIR}/mlb.pkl', 'wb') as f:\n    pickle.dump(mlb, f)\n\nsparse.save_npz(f'{MODEL_DIR}/X_train.npz', X_train)\nsparse.save_npz(f'{MODEL_DIR}/Y_train.npz', Y_train)\n\nprint(\"✅ ĐÃ TRAIN VÀ LƯU XONG! BẠN CÓ THỂ RESET KERNEL ĐỂ CHẠY PHẦN 2.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:35.747054Z","iopub.execute_input":"2025-12-13T10:36:35.747261Z","iopub.status.idle":"2025-12-13T10:38:10.497045Z","shell.execute_reply.started":"2025-12-13T10:36:35.747243Z","shell.execute_reply":"2025-12-13T10:38:10.496377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#prediction\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport gc\nimport os\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nBASE_PATH = '/kaggle/input/cafa-6-protein-function-prediction'\nMODEL_DIR = '/kaggle/working'\nSUBMISSION_FILE = 'submission.tsv'\nBATCH_SIZE = 1000\nTOP_K = 25             \nTAXON_BONUS = 1.3       \n\nprint(\"--- BẮT ĐẦU: BIO-ENHANCED PREDICTION ---\")\n\nprint(\"1. Loading Models...\")\nwith open(f'{MODEL_DIR}/vectorizer.pkl', 'rb') as f:\n    vectorizer = pickle.load(f)\nwith open(f'{MODEL_DIR}/mlb.pkl', 'rb') as f:\n    mlb = pickle.load(f)\n    \nX_train = sparse.load_npz(f'{MODEL_DIR}/X_train.npz')\nY_train = sparse.load_npz(f'{MODEL_DIR}/Y_train.npz')\nall_terms = mlb.classes_\nprint(f\"   -> Model loaded. X_train shape: {X_train.shape}\")\n\nprint(\"2. Loading Taxonomy & Lengths (Train)...\")\ntrain_tax_df = pd.read_csv(f\"{BASE_PATH}/Train/train_taxonomy.tsv\", sep='\\t', header=None, names=['ID', 'TaxID'])\ntrain_tax_map = dict(zip(train_tax_df['ID'], train_tax_df['TaxID']))\n\ndef load_train_metadata():\n    # Load raw\n    seqs = {}\n    with open(f\"{BASE_PATH}/Train/train_sequences.fasta\", 'r') as f:\n        cid, cseq = \"\", []\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if cid: seqs[cid] = len(\"\".join(cseq))\n                parts = line.split('|')\n                cid = parts[1] if len(parts) > 1 else line[1:].split()[0]\n                cseq = []\n            else:\n                cseq.append(line)\n        if cid: seqs[cid] = len(\"\".join(cseq))\n    \n    df_terms = pd.read_csv(f\"{BASE_PATH}/Train/train_terms.tsv\", sep='\\t')\n    valid_ids_with_labels = set(df_terms['EntryID'].unique())\n        \n    final_ids = [pid for pid in seqs.keys() if pid in valid_ids_with_labels]\n    \n    tax_arr = np.array([train_tax_map.get(pid, 0) for pid in final_ids], dtype=np.int32)\n    len_arr = np.array([seqs[pid] for pid in final_ids], dtype=np.float32)\n    return tax_arr, len_arr\n\ntrain_tax_arr, train_len_arr = load_train_metadata()\nprint(f\"   -> Metadata loaded. Count: {len(train_tax_arr)}\")\n\nif len(train_tax_arr) != X_train.shape[0]:\n    print(f\"⚠️ CẢNH BÁO: Số lượng metadata ({len(train_tax_arr)}) khác số lượng vector ({X_train.shape[0]}).\")\n    print(\"   -> Điều này sẽ gây lỗi lệch hàng. Hãy đảm bảo Logic lọc ở Part 1 và Part 2 giống hệt nhau!\")\n\nprint(\"3. Loading Test Data...\")\ndef read_test_fasta(path):\n    data = [] # List of tuples (id, seq, tax, len)\n    with open(path, 'r') as f:\n        cid, cseq, ctax = \"\", [], 0\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if cid: \n                    full_seq = \"\".join(cseq)\n                    data.append((cid, full_seq, ctax, len(full_seq)))\n                \n                parts = line[1:].split()\n                cid = parts[0]\n                ctax = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n                cseq = []\n            else:\n                cseq.append(line)\n        if cid:\n            full_seq = \"\".join(cseq)\n            data.append((cid, full_seq, ctax, len(full_seq)))\n    return data\n\ntest_data = read_test_fasta(f\"{BASE_PATH}/Test/testsuperset.fasta\")\n# Tách ra để xử lý batch\ntest_ids = [x[0] for x in test_data]\ntest_seqs = [x[1] for x in test_data]\ntest_tax = np.array([x[2] for x in test_data], dtype=np.int32)\ntest_lens = np.array([x[3] for x in test_data], dtype=np.float32)\n\ndel test_data\ngc.collect()\n\nprint(\"4. Running Prediction with Bio-Heuristics...\")\nn_test = len(test_ids)\nn_batches = int(np.ceil(n_test / BATCH_SIZE))\n\nif os.path.exists(SUBMISSION_FILE): os.remove(SUBMISSION_FILE)\n\nfor b in range(n_batches):\n    start = b * BATCH_SIZE\n    end = min(n_test, (b + 1) * BATCH_SIZE)\n    \n    b_ids = test_ids[start:end]\n    b_seqs = test_seqs[start:end]\n    b_tax = test_tax[start:end]\n    b_lens = test_lens[start:end]\n    \n    X_test = vectorizer.transform(b_seqs)\n    sim_matrix = cosine_similarity(X_test, X_train)\n    \n    batch_rows = []\n    \n    for i in range(len(b_ids)):\n        # Top K\n        best_indices = np.argpartition(sim_matrix[i], -TOP_K)[-TOP_K:]\n        scores = sim_matrix[i][best_indices]\n        neighbor_indices = best_indices # Alias\n        \n        # heuristic 1: Taxonomy Bonus\n        current_tax = b_tax[i]\n        if current_tax != 0:\n            neighbor_taxs = train_tax_arr[neighbor_indices]\n            # Tạo mask những thằng trùng tax\n            tax_match = (neighbor_taxs == current_tax)\n            scores[tax_match] *= TAXON_BONUS\n            \n        # heuristic 2: Length Penalty \n        current_len = b_lens[i]\n        neighbor_lens = train_len_arr[neighbor_indices]\n        \n        min_l = np.minimum(neighbor_lens, current_len)\n        max_l = np.maximum(neighbor_lens, current_len)\n        ratio = min_l / (max_l + 1e-5) # Tránh chia 0\n        \n        scores *= ratio\n        \n        sum_sim = np.sum(scores)\n        if sum_sim > 0.001:\n            neighbor_labels_mat = Y_train[neighbor_indices]\n            w_scores = sparse.csr_matrix(scores)\n            \n            pred_scores = w_scores.dot(neighbor_labels_mat).toarray().flatten() / sum_sim\n            \n            top_term_indices = np.argsort(pred_scores)[-50:]\n            \n            for idx in top_term_indices:\n                s = pred_scores[idx]\n                if s > 0.01:\n                    batch_rows.append(f\"{b_ids[i]}\\t{all_terms[idx]}\\t{s:.3f}\")\n    \n    with open(SUBMISSION_FILE, 'a') as f:\n        if batch_rows:\n            f.write('\\n'.join(batch_rows) + '\\n')\n            \n    if (b+1) % 10 == 0:\n        print(f\"   Batch {b+1}/{n_batches} xong.\")\n        gc.collect()\n\nprint(f\"✅ HOÀN TẤT! File kết quả: {SUBMISSION_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:38:10.497796Z","iopub.execute_input":"2025-12-13T10:38:10.497987Z","iopub.status.idle":"2025-12-13T11:29:10.595429Z","shell.execute_reply.started":"2025-12-13T10:38:10.497964Z","shell.execute_reply":"2025-12-13T11:29:10.594797Z"}},"outputs":[],"execution_count":null}]}