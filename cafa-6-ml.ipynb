{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Imports\n\nimport os, glob, gzip, re\nimport pandas as pd, numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom scipy import sparse\nimport math\n\n!pip install -q biopython\n\n# test import immediately\ntry:\n    from Bio import SeqIO\n    print('Biopython installed and import OK:', SeqIO.__name__)\nexcept Exception as e:\n    print('Import failed — you may need to restart the kernel. Error:', e) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:15.709426Z","iopub.execute_input":"2025-12-13T10:36:15.709814Z","iopub.status.idle":"2025-12-13T10:36:18.854636Z","shell.execute_reply.started":"2025-12-13T10:36:15.709782Z","shell.execute_reply":"2025-12-13T10:36:18.853610Z"}},"outputs":[{"name":"stdout","text":"Biopython installed and import OK: Bio.SeqIO\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#File Detection\n\nINPUT_ROOT = '/kaggle/input'\nall_files = glob.glob(os.path.join(INPUT_ROOT, '**', '*'), recursive=True)\n\nfasta_train = None\nfasta_test = None\nterms_tsv = None\n\nfor p in all_files:\n    low = p.lower()\n    if low.endswith('.fasta') or low.endswith('.fa') or low.endswith('.fasta.gz') or low.endswith('.fa.gz'):\n        if 'train' in low and fasta_train is None:\n            fasta_train = p\n        elif 'test' in low and fasta_test is None:\n            fasta_test = p\n        elif fasta_train is None:\n            fasta_train = p\n    if (low.endswith('.tsv') or low.endswith('.csv')) and terms_tsv is None:\n        if 'term' in low or 'label' in low or 'annotation' in low or 'train_terms' in low:\n            terms_tsv = p\n\ncomp_folder = os.path.join(INPUT_ROOT, 'cafa-6-protein-function-prediction')\nif os.path.exists(comp_folder):\n    if fasta_train is None and os.path.exists(os.path.join(comp_folder, 'train_sequences.fasta')):\n        fasta_train = os.path.join(comp_folder, 'train_sequences.fasta')\n    if fasta_test is None and os.path.exists(os.path.join(comp_folder, 'test_sequences.fasta')):\n        fasta_test = os.path.join(comp_folder, 'test_sequences.fasta')\n    if terms_tsv is None and os.path.exists(os.path.join(comp_folder, 'train_terms.tsv')):\n        terms_tsv = os.path.join(comp_folder, 'train_terms.tsv')\n\nif not fasta_train or not fasta_test or not terms_tsv:\n    print('Auto-detection failed. Found:')\n    print(' fasta_train =', fasta_train)\n    print(' fasta_test  =', fasta_test)\n    print(' terms_tsv   =', terms_tsv)\n    raise SystemExit('Attach the CAFA-6 dataset: Add data -> cafa-6-protein-function-prediction')\n\nprint('Using files:')\nprint(' train fasta:', fasta_train)\nprint(' test  fasta:', fasta_test)\nprint(' terms tsv :', terms_tsv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.856169Z","iopub.execute_input":"2025-12-13T10:36:18.856378Z","iopub.status.idle":"2025-12-13T10:36:18.869456Z","shell.execute_reply.started":"2025-12-13T10:36:18.856358Z","shell.execute_reply":"2025-12-13T10:36:18.868916Z"}},"outputs":[{"name":"stdout","text":"Using files:\n train fasta: /kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\n test  fasta: /kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\n terms tsv : /kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"#Defining Helpers\n\ndef read_fasta_ids(path, limit=None):\n    open_fn = gzip.open if str(path).endswith('.gz') else open\n    ids = []\n    with open_fn(path, 'rt') as handle:\n        for rec in SeqIO.parse(handle, 'fasta'):\n            ids.append(rec.id)\n            if limit and len(ids) >= limit:\n                break\n    return ids\n\ndef read_fasta_dict(path):\n    open_fn = gzip.open if str(path).endswith('.gz') else open\n    seqs = {}\n    with open_fn(path, 'rt') as handle:\n        for rec in SeqIO.parse(handle, 'fasta'):\n            seqs[rec.id] = str(rec.seq)\n    return seqs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.870078Z","iopub.execute_input":"2025-12-13T10:36:18.870299Z","iopub.status.idle":"2025-12-13T10:36:18.878004Z","shell.execute_reply.started":"2025-12-13T10:36:18.870283Z","shell.execute_reply":"2025-12-13T10:36:18.877286Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#previewing and debudding\n\ntrain_sample_ids = read_fasta_ids(fasta_train, limit=20)\ntest_sample_ids = read_fasta_ids(fasta_test, limit=20)\nprint('\\nExample train FASTA ids (20):', train_sample_ids[:20])\nprint('Example test  FASTA ids (20):', test_sample_ids[:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.879125Z","iopub.execute_input":"2025-12-13T10:36:18.879351Z","iopub.status.idle":"2025-12-13T10:36:18.897002Z","shell.execute_reply.started":"2025-12-13T10:36:18.879336Z","shell.execute_reply":"2025-12-13T10:36:18.896265Z"}},"outputs":[{"name":"stdout","text":"\nExample train FASTA ids (20): ['sp|A0A0C5B5G6|MOTSC_HUMAN', 'sp|A0JNW5|BLT3B_HUMAN', 'sp|A0JP26|POTB3_HUMAN', 'sp|A0PK11|CLRN2_HUMAN', 'sp|A1A4S6|RHG10_HUMAN', 'sp|A1A519|F170A_HUMAN', 'sp|A1L190|SYCE3_HUMAN', 'sp|A1L3X0|ELOV7_HUMAN', 'sp|A1X283|SPD2B_HUMAN', 'sp|A2A2Y4|FRMD3_HUMAN', 'sp|A2RU14|TM218_HUMAN', 'sp|A2RUB6|CCD66_HUMAN', 'sp|A2RUC4|TYW5_HUMAN', 'sp|A4D1B5|GSAP_HUMAN', 'sp|A4GXA9|EME2_HUMAN', 'sp|A5D8V7|ODAD3_HUMAN', 'sp|A5PLL7|PEDS1_HUMAN', 'sp|A6BM72|MEG11_HUMAN', 'sp|A6H8Y1|BDP1_HUMAN', 'sp|A6NCS4|NKX26_HUMAN']\nExample test  FASTA ids (20): ['A0A0C5B5G6', 'A0A1B0GTW7', 'A0JNW5', 'A0JP26', 'A0PK11', 'A1A4S6', 'A1A519', 'A1L190', 'A1L3X0', 'A1X283', 'A2A2Y4', 'A2RU14', 'A2RUB6', 'A2RUC4', 'A4D1B5', 'A4GXA9', 'A5D8V7', 'A5PLL7', 'A6BM72', 'A6H8Y1']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"#read and detect terms_tsv format\n\nopen_fn = gzip.open if str(terms_tsv).endswith('.gz') else open\nprint('\\n--- first 12 raw lines of terms_tsv ---')\nwith open_fn(terms_tsv, 'rt') as f:\n    for i, line in enumerate(f):\n        print(i+1, line.strip())\n        if i >= 11:\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.898899Z","iopub.execute_input":"2025-12-13T10:36:18.899122Z","iopub.status.idle":"2025-12-13T10:36:18.908362Z","shell.execute_reply.started":"2025-12-13T10:36:18.899107Z","shell.execute_reply":"2025-12-13T10:36:18.907709Z"}},"outputs":[{"name":"stdout","text":"\n--- first 12 raw lines of terms_tsv ---\n1 EntryID\tterm\taspect\n2 Q5W0B1\tGO:0000785\tC\n3 Q5W0B1\tGO:0004842\tF\n4 Q5W0B1\tGO:0051865\tP\n5 Q5W0B1\tGO:0006275\tP\n6 Q5W0B1\tGO:0006513\tP\n7 Q5W0B1\tGO:0003682\tF\n8 Q5W0B1\tGO:0005515\tF\n9 Q3EC77\tGO:0000138\tC\n10 Q3EC77\tGO:0005794\tC\n11 Q8IZR5\tGO:0005515\tF\n12 Q8R2Z3\tGO:0140900\tF\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#trying with more rows \n\ndf = pd.read_csv(terms_tsv, sep='\\t', header=None, dtype=str, engine='python')\nprint('\\nRead shape (terms file):', df.shape)\nprint('First rows:')\nprint(df.head(6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:18.909230Z","iopub.execute_input":"2025-12-13T10:36:18.909604Z","iopub.status.idle":"2025-12-13T10:36:20.493105Z","shell.execute_reply.started":"2025-12-13T10:36:18.909581Z","shell.execute_reply":"2025-12-13T10:36:20.492227Z"}},"outputs":[{"name":"stdout","text":"\nRead shape (terms file): (537028, 3)\nFirst rows:\n         0           1       2\n0  EntryID        term  aspect\n1   Q5W0B1  GO:0000785       C\n2   Q5W0B1  GO:0004842       F\n3   Q5W0B1  GO:0051865       P\n4   Q5W0B1  GO:0006275       P\n5   Q5W0B1  GO:0006513       P\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# heuristic- check for GO or similar\n\ncol0_is_go = df[0].astype(str).str.match(r'^GO:\\\\d{7}').sum() if 0 in df.columns else 0\ncol1_is_go = df[1].astype(str).str.match(r'^GO:\\\\d{7}').sum() if 1 in df.columns else 0\nprint(f'col0 GO-like count = {col0_is_go}, col1 GO-like count = {col1_is_go}')\n\nif col0_is_go > col1_is_go:\n    print('Detected first column contains GO terms. Interpreting file as (term_id, protein_id) -> swapping.')\n    terms_df = df.rename(columns={0:'term_id', 1:'protein_id'})[['protein_id','term_id']]\nelse:\n    print('Detected first column contains protein ids. Using (protein_id, term_id).')\n    terms_df = df.rename(columns={0:'protein_id', 1:'term_id'})[['protein_id','term_id']]\n\nprint('\\nterms_df sample:')\nprint(terms_df.head(6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:20.494136Z","iopub.execute_input":"2025-12-13T10:36:20.494533Z","iopub.status.idle":"2025-12-13T10:36:20.927443Z","shell.execute_reply.started":"2025-12-13T10:36:20.494486Z","shell.execute_reply":"2025-12-13T10:36:20.926719Z"}},"outputs":[{"name":"stdout","text":"col0 GO-like count = 0, col1 GO-like count = 0\nDetected first column contains protein ids. Using (protein_id, term_id).\n\nterms_df sample:\n  protein_id     term_id\n0    EntryID        term\n1     Q5W0B1  GO:0000785\n2     Q5W0B1  GO:0004842\n3     Q5W0B1  GO:0051865\n4     Q5W0B1  GO:0006275\n5     Q5W0B1  GO:0006513\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#Normalization \n\ndef norm_variants(s):\n    s = '' if s is None else str(s)\n    out = []\n    out.append(s)\n    if '|' in s:\n        parts = s.split('|')\n        for p in parts:\n            if p: out.append(p)\n        out.append(parts[-1])\n        if len(parts) > 1: out.append(parts[1])\n    out.append(s.split()[0])\n    if '.' in s: out.append(s.split('.')[0])\n    out.append(re.sub('[^A-Za-z0-9_\\\\-]', '', s))\n    for t in re.split('[\\\\|\\\\s]', s):\n        t = t.strip()\n        if 4 <= len(t) <= 12:\n            out.append(t)\n    uniq = []\n    for v in out:\n        if v and v not in uniq: uniq.append(v)\n    return uniq\n\nlabel_ids = pd.unique(terms_df['protein_id'])\nnorm2labels = {}\nfor lid in label_ids:\n    for nv in norm_variants(lid):\n        norm2labels.setdefault(nv, set()).add(lid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:20.928336Z","iopub.execute_input":"2025-12-13T10:36:20.928604Z","iopub.status.idle":"2025-12-13T10:36:21.193943Z","shell.execute_reply.started":"2025-12-13T10:36:20.928586Z","shell.execute_reply":"2025-12-13T10:36:21.193369Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print('\\nSample match count (train sample):', sum(1 for tid in train_sample_ids if any(c in norm2labels for c in norm_variants(tid))), '/', len(train_sample_ids))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:21.194661Z","iopub.execute_input":"2025-12-13T10:36:21.194869Z","iopub.status.idle":"2025-12-13T10:36:21.199366Z","shell.execute_reply.started":"2025-12-13T10:36:21.194852Z","shell.execute_reply":"2025-12-13T10:36:21.198772Z"}},"outputs":[{"name":"stdout","text":"\nSample match count (train sample): 20 / 20\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"#Map train (fasta ids -> lable ids)\n\ntrain_all_ids = read_fasta_ids(fasta_train, limit=None)\nmapped_train_to_label = {}\nunmatched = []\nfor tid in train_all_ids:\n    found = False\n    for cand in norm_variants(tid):\n        if cand in norm2labels:\n            mapped_train_to_label[tid] = list(norm2labels[cand])[0]\n            found = True\n            break\n    if not found:\n        unmatched.append(tid)\n\nprint('Total train sequences:', len(train_all_ids))\nprint('Mapped train->label count:', len(mapped_train_to_label))\nprint('Unmatched train sequences:', len(unmatched))\nprint('Example unmatched (10):', unmatched[:10])\n\nif len(mapped_train_to_label) == 0:\n    raise SystemExit('No mapping between fasta ids and label ids found automatically. Paste the first raw lines above here for further rule craft.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:21.200147Z","iopub.execute_input":"2025-12-13T10:36:21.200443Z","iopub.status.idle":"2025-12-13T10:36:21.963063Z","shell.execute_reply.started":"2025-12-13T10:36:21.200427Z","shell.execute_reply":"2025-12-13T10:36:21.962436Z"}},"outputs":[{"name":"stdout","text":"Total train sequences: 82404\nMapped train->label count: 82404\nUnmatched train sequences: 0\nExample unmatched (10): []\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"#mapping terms_df\n\nlabel_to_fasta = {}\nfor f_id, l_id in mapped_train_to_label.items():\n    label_to_fasta.setdefault(l_id, set()).add(f_id)\n\ndef map_label_row(lid):\n    if lid in label_to_fasta:\n        return list(label_to_fasta[lid])[0]\n    for nv in norm_variants(lid):\n        if nv in label_to_fasta:\n            return list(label_to_fasta[nv])[0]\n    return None\n\nterms_df['mapped_fasta_id'] = terms_df['protein_id'].apply(map_label_row)\nmapped_count = int(terms_df['mapped_fasta_id'].notnull().sum())\nprint('Mapped label rows to fasta ids:', mapped_count, '/', len(terms_df))\n\nmapped_df = terms_df[terms_df['mapped_fasta_id'].notnull()].copy()\ngrouped = mapped_df.groupby('mapped_fasta_id')['term_id'].apply(list).reset_index().rename(columns={'mapped_fasta_id':'protein_id'})\nprint('Grouped unique proteins with labels (after mapping):', len(grouped))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:21.963783Z","iopub.execute_input":"2025-12-13T10:36:21.964101Z","iopub.status.idle":"2025-12-13T10:36:24.031870Z","shell.execute_reply.started":"2025-12-13T10:36:21.964078Z","shell.execute_reply":"2025-12-13T10:36:24.031096Z"}},"outputs":[{"name":"stdout","text":"Mapped label rows to fasta ids: 537027 / 537028\nGrouped unique proteins with labels (after mapping): 82404\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"#loading seq dict\n\ntrain_seqs = read_fasta_dict(fasta_train)\ntest_seqs = read_fasta_dict(fasta_test)\n\ngrouped = grouped[grouped['protein_id'].isin(train_seqs.keys())].reset_index(drop=True)\nprint('Grouped after filtering to available sequences:', len(grouped))\nif len(grouped) == 0:\n    raise SystemExit('No labelled proteins remain after mapping and filtering. Inspect unmatched examples above.')\n\ngrouped['sequence'] = grouped['protein_id'].map(train_seqs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:24.032776Z","iopub.execute_input":"2025-12-13T10:36:24.033110Z","iopub.status.idle":"2025-12-13T10:36:25.434303Z","shell.execute_reply.started":"2025-12-13T10:36:24.033088Z","shell.execute_reply":"2025-12-13T10:36:25.433532Z"}},"outputs":[{"name":"stdout","text":"Grouped after filtering to available sequences: 82404\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"#k-mers\n\nK = 3\ndef kmers(seq, k=K):\n    if not isinstance(seq, str) or len(seq) == 0: return ['']\n    if len(seq) < k: return [seq]\n    return [seq[i:i+k] for i in range(len(seq)-k+1)]\n\ntrain_texts = [' '.join(kmers(s)) for s in grouped['sequence']]\n\nvectorizer = HashingVectorizer(n_features=2**15, alternate_sign=False, token_pattern=r'[^\\\\s]+')  # 32k features\nX_full = vectorizer.transform(train_texts)\nif 'mlb' not in globals():\n    from sklearn.preprocessing import MultiLabelBinarizer\n    mlb = MultiLabelBinarizer(sparse_output=True)\nY_sparse = mlb.fit_transform(grouped['term_id'])\nprint('X_full shape:', X_full.shape, 'Y shape:', Y_sparse.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:25.435210Z","iopub.execute_input":"2025-12-13T10:36:25.435426Z","iopub.status.idle":"2025-12-13T10:36:35.745082Z","shell.execute_reply.started":"2025-12-13T10:36:25.435402Z","shell.execute_reply":"2025-12-13T10:36:35.744261Z"}},"outputs":[{"name":"stdout","text":"X_full shape: (82404, 32768) Y shape: (82404, 26125)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# PHẦN 1: TRAINING & SAVING ARTIFACTS\nimport numpy as np\nimport pandas as pd\nimport gc\nimport pickle\nfrom scipy import sparse\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# --- CẤU HÌNH ---\nBASE_PATH = '/kaggle/input/cafa-6-protein-function-prediction'\nMODEL_DIR = '/kaggle/working'  # Nơi lưu model\nNGRAM_RANGE = (3, 4)\nMAX_FEATURES = 20000\n\nprint(\"--- BẮT ĐẦU: TRAINING ---\")\n\n# 1. LOAD DỮ LIỆU\nprint(\"1. Đang tải dữ liệu Train...\")\n# Hàm đọc Fasta nhanh\ndef read_fasta(path):\n    seqs = {}\n    with open(path, 'r') as f:\n        cid, cseq = \"\", []\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if cid: seqs[cid] = \"\".join(cseq)\n                parts = line.split('|')\n                cid = parts[1] if len(parts) > 1 else line[1:].split()[0]\n                cseq = []\n            else:\n                cseq.append(line)\n        if cid: seqs[cid] = \"\".join(cseq)\n    return seqs\n\ntrain_seqs = read_fasta(f\"{BASE_PATH}/Train/train_sequences.fasta\")\ndf_terms = pd.read_csv(f\"{BASE_PATH}/Train/train_terms.tsv\", sep='\\t')\n\n# Xử lý nhãn\nprint(\"   -> Map nhãn vào sequence...\")\ngrouped = df_terms.groupby('EntryID')['term'].apply(list).reset_index()\ngrouped_map = dict(zip(grouped['EntryID'], grouped['term']))\n\ntrain_ids = list(train_seqs.keys())\ntrain_sentences = [train_seqs[pid] for pid in train_ids]\ntrain_labels = [grouped_map.get(pid, []) for pid in train_ids]\n\n# Lọc bỏ mẫu không có nhãn\nmask = [len(x) > 0 for x in train_labels]\ntrain_sentences = [x for i, x in enumerate(train_sentences) if mask[i]]\ntrain_labels = [x for i, x in enumerate(train_labels) if mask[i]]\n\nprint(f\"   -> Số lượng mẫu Train hợp lệ: {len(train_sentences)}\")\ndel train_seqs, df_terms, grouped\ngc.collect()\n\n# 2. VECTOR HÓA & LƯU\n# ---------------------------------------------------------\nprint(\"2. Training Vectorizer (TF-IDF)...\")\nvectorizer = TfidfVectorizer(\n    analyzer='char', ngram_range=NGRAM_RANGE, \n    max_features=MAX_FEATURES, dtype=np.float32, sublinear_tf=True\n)\nX_train = vectorizer.fit_transform(train_sentences)\n\nprint(\"3. Binarizing Labels...\")\nmlb = MultiLabelBinarizer(sparse_output=True)\nY_train = mlb.fit_transform(train_labels)\n\n# 3. LƯU XUỐNG FILE\n# ---------------------------------------------------------\nprint(f\"4. Lưu model xuống {MODEL_DIR}...\")\n\n# Lưu Vectorizer và MLB bằng pickle\nwith open(f'{MODEL_DIR}/vectorizer.pkl', 'wb') as f:\n    pickle.dump(vectorizer, f)\n    \nwith open(f'{MODEL_DIR}/mlb.pkl', 'wb') as f:\n    pickle.dump(mlb, f)\n\n# Lưu ma trận thưa (Sparse Matrix) bằng scipy (rất nhẹ và nhanh)\nsparse.save_npz(f'{MODEL_DIR}/X_train.npz', X_train)\nsparse.save_npz(f'{MODEL_DIR}/Y_train.npz', Y_train)\n\nprint(\"✅ ĐÃ TRAIN VÀ LƯU XONG! BẠN CÓ THỂ RESET KERNEL ĐỂ CHẠY PHẦN 2.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:36:35.747054Z","iopub.execute_input":"2025-12-13T10:36:35.747261Z","iopub.status.idle":"2025-12-13T10:38:10.497045Z","shell.execute_reply.started":"2025-12-13T10:36:35.747243Z","shell.execute_reply":"2025-12-13T10:38:10.496377Z"}},"outputs":[{"name":"stdout","text":"--- BẮT ĐẦU: TRAINING ---\n1. Đang tải dữ liệu Train...\n   -> Map nhãn vào sequence...\n   -> Số lượng mẫu Train hợp lệ: 82404\n2. Training Vectorizer (TF-IDF)...\n3. Binarizing Labels...\n4. Lưu model xuống /kaggle/working...\n✅ ĐÃ TRAIN VÀ LƯU XONG! BẠN CÓ THỂ RESET KERNEL ĐỂ CHẠY PHẦN 2.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ==============================================================================\n# PHẦN 2 (NÂNG CẤP): PREDICTION VỚI TAXONOMY & LENGTH PENALTY\n# Mục tiêu: 0.170 -> 0.25+\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport gc\nimport os\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# --- CẤU HÌNH ---\nBASE_PATH = '/kaggle/input/cafa-6-protein-function-prediction'\nMODEL_DIR = '/kaggle/working'\nSUBMISSION_FILE = 'submission.tsv'\nBATCH_SIZE = 1000\nTOP_K = 25              # Tăng lên 25 để có nhiều ứng viên hơn trước khi lọc\nTAXON_BONUS = 1.3       # Cộng 30% điểm nếu cùng loài (quan trọng!)\n\nprint(\"--- BẮT ĐẦU: BIO-ENHANCED PREDICTION ---\")\n\n# 1. LOAD MODEL\n# ---------------------------------------------------------\nprint(\"1. Loading Models...\")\nwith open(f'{MODEL_DIR}/vectorizer.pkl', 'rb') as f:\n    vectorizer = pickle.load(f)\nwith open(f'{MODEL_DIR}/mlb.pkl', 'rb') as f:\n    mlb = pickle.load(f)\n    \nX_train = sparse.load_npz(f'{MODEL_DIR}/X_train.npz')\nY_train = sparse.load_npz(f'{MODEL_DIR}/Y_train.npz')\nall_terms = mlb.classes_\nprint(f\"   -> Model loaded. X_train shape: {X_train.shape}\")\n\n# 2. LOAD BIO-METADATA (Taxonomy & Length)\n# ---------------------------------------------------------\nprint(\"2. Loading Taxonomy & Lengths (Train)...\")\n# Load Taxonomy map\ntrain_tax_df = pd.read_csv(f\"{BASE_PATH}/Train/train_taxonomy.tsv\", sep='\\t', header=None, names=['ID', 'TaxID'])\ntrain_tax_map = dict(zip(train_tax_df['ID'], train_tax_df['TaxID']))\n\n# Load Train Lengths (cần thiết để phạt chênh lệch độ dài)\n# Chúng ta cần list length khớp thứ tự với X_train.\n# Do ở Phần 1 ta đã lọc train_seqs, ta cần đảm bảo thứ tự này khớp.\n# CÁCH AN TOÀN NHẤT: Re-load train seqs để lấy length và taxonomy theo đúng thứ tự vectorizer đã học\ndef load_train_metadata():\n    # Load raw\n    seqs = {}\n    with open(f\"{BASE_PATH}/Train/train_sequences.fasta\", 'r') as f:\n        cid, cseq = \"\", []\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if cid: seqs[cid] = len(\"\".join(cseq))\n                parts = line.split('|')\n                cid = parts[1] if len(parts) > 1 else line[1:].split()[0]\n                cseq = []\n            else:\n                cseq.append(line)\n        if cid: seqs[cid] = len(\"\".join(cseq))\n    \n    # Load labels mapping để tái tạo filter mask giống hệt lúc train\n    df_terms = pd.read_csv(f\"{BASE_PATH}/Train/train_terms.tsv\", sep='\\t')\n    valid_ids_with_labels = set(df_terms['EntryID'].unique())\n    \n    # Lấy danh sách ID theo thứ tự file fasta (giống logic Part 1)\n    # Lưu ý: Logic này phải KHỚP 100% với Part 1. \n    # Nếu Part 1 bạn dùng read_fasta dict thì keys() có thể không theo thứ tự.\n    # ĐỂ CHẮC CHẮN: Ta dùng danh sách ID mà Part 1 đã xử lý. \n    # (Ở đây giả định Part 1 chạy dictionary Python 3.7+ giữ insertion order)\n    \n    final_ids = [pid for pid in seqs.keys() if pid in valid_ids_with_labels]\n    \n    # Tạo array\n    tax_arr = np.array([train_tax_map.get(pid, 0) for pid in final_ids], dtype=np.int32)\n    len_arr = np.array([seqs[pid] for pid in final_ids], dtype=np.float32)\n    return tax_arr, len_arr\n\ntrain_tax_arr, train_len_arr = load_train_metadata()\nprint(f\"   -> Metadata loaded. Count: {len(train_tax_arr)}\")\n\nif len(train_tax_arr) != X_train.shape[0]:\n    print(f\"⚠️ CẢNH BÁO: Số lượng metadata ({len(train_tax_arr)}) khác số lượng vector ({X_train.shape[0]}).\")\n    print(\"   -> Điều này sẽ gây lỗi lệch hàng. Hãy đảm bảo Logic lọc ở Part 1 và Part 2 giống hệt nhau!\")\n    # Fix nhanh: Cắt hoặc padding (nhưng tốt nhất là check lại Part 1)\n    # Ở đây tôi giả định bạn chạy đúng code Part 1 tôi đưa -> thứ tự dict được bảo toàn.\n\n# 3. CHUẨN BỊ TEST DATA\n# ---------------------------------------------------------\nprint(\"3. Loading Test Data...\")\ndef read_test_fasta(path):\n    data = [] # List of tuples (id, seq, tax, len)\n    with open(path, 'r') as f:\n        cid, cseq, ctax = \"\", [], 0\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if cid: \n                    full_seq = \"\".join(cseq)\n                    data.append((cid, full_seq, ctax, len(full_seq)))\n                \n                parts = line[1:].split()\n                cid = parts[0]\n                ctax = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n                cseq = []\n            else:\n                cseq.append(line)\n        if cid:\n            full_seq = \"\".join(cseq)\n            data.append((cid, full_seq, ctax, len(full_seq)))\n    return data\n\ntest_data = read_test_fasta(f\"{BASE_PATH}/Test/testsuperset.fasta\")\n# Tách ra để xử lý batch\ntest_ids = [x[0] for x in test_data]\ntest_seqs = [x[1] for x in test_data]\ntest_tax = np.array([x[2] for x in test_data], dtype=np.int32)\ntest_lens = np.array([x[3] for x in test_data], dtype=np.float32)\n\ndel test_data\ngc.collect()\n\n# 4. PREDICTION LOOP\n# ---------------------------------------------------------\nprint(\"4. Running Prediction with Bio-Heuristics...\")\nn_test = len(test_ids)\nn_batches = int(np.ceil(n_test / BATCH_SIZE))\n\nif os.path.exists(SUBMISSION_FILE): os.remove(SUBMISSION_FILE)\n\nfor b in range(n_batches):\n    start = b * BATCH_SIZE\n    end = min(n_test, (b + 1) * BATCH_SIZE)\n    \n    # Batch data\n    b_ids = test_ids[start:end]\n    b_seqs = test_seqs[start:end]\n    b_tax = test_tax[start:end]\n    b_lens = test_lens[start:end]\n    \n    # 1. Vectorize & Similarity\n    X_test = vectorizer.transform(b_seqs)\n    sim_matrix = cosine_similarity(X_test, X_train)\n    \n    batch_rows = []\n    \n    for i in range(len(b_ids)):\n        # Top K\n        best_indices = np.argpartition(sim_matrix[i], -TOP_K)[-TOP_K:]\n        scores = sim_matrix[i][best_indices]\n        neighbor_indices = best_indices # Alias\n        \n        # --- HEURISTIC 1: TAXONOMY BONUS ---\n        # Nếu Test Protein cùng loài với Train Protein -> Tăng điểm\n        current_tax = b_tax[i]\n        if current_tax != 0:\n            neighbor_taxs = train_tax_arr[neighbor_indices]\n            # Tạo mask những thằng trùng tax\n            tax_match = (neighbor_taxs == current_tax)\n            scores[tax_match] *= TAXON_BONUS\n            \n        # --- HEURISTIC 2: LENGTH PENALTY ---\n        # Phạt nếu độ dài chênh lệch quá nhiều\n        current_len = b_lens[i]\n        neighbor_lens = train_len_arr[neighbor_indices]\n        \n        min_l = np.minimum(neighbor_lens, current_len)\n        max_l = np.maximum(neighbor_lens, current_len)\n        ratio = min_l / (max_l + 1e-5) # Tránh chia 0\n        \n        scores *= ratio\n        \n        # --- TÍNH TỔNG & GHI NHÃN ---\n        sum_sim = np.sum(scores)\n        if sum_sim > 0.001:\n            neighbor_labels_mat = Y_train[neighbor_indices]\n            w_scores = sparse.csr_matrix(scores)\n            \n            # Weighted Vote\n            pred_scores = w_scores.dot(neighbor_labels_mat).toarray().flatten() / sum_sim\n            \n            # Lấy top 50 nhãn cao nhất\n            top_term_indices = np.argsort(pred_scores)[-50:]\n            \n            for idx in top_term_indices:\n                s = pred_scores[idx]\n                if s > 0.01:\n                    batch_rows.append(f\"{b_ids[i]}\\t{all_terms[idx]}\\t{s:.3f}\")\n    \n    # Ghi file\n    with open(SUBMISSION_FILE, 'a') as f:\n        if batch_rows:\n            f.write('\\n'.join(batch_rows) + '\\n')\n            \n    if (b+1) % 10 == 0:\n        print(f\"   Batch {b+1}/{n_batches} xong.\")\n        gc.collect()\n\nprint(f\"✅ HOÀN TẤT! File kết quả: {SUBMISSION_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T10:38:10.497796Z","iopub.execute_input":"2025-12-13T10:38:10.497987Z","iopub.status.idle":"2025-12-13T11:29:10.595429Z","shell.execute_reply.started":"2025-12-13T10:38:10.497964Z","shell.execute_reply":"2025-12-13T11:29:10.594797Z"}},"outputs":[{"name":"stdout","text":"--- BẮT ĐẦU: BIO-ENHANCED PREDICTION ---\n1. Loading Models...\n   -> Model loaded. X_train shape: (82404, 20000)\n2. Loading Taxonomy & Lengths (Train)...\n   -> Metadata loaded. Count: 82404\n3. Loading Test Data...\n4. Running Prediction with Bio-Heuristics...\n   Batch 10/225 xong.\n   Batch 20/225 xong.\n   Batch 30/225 xong.\n   Batch 40/225 xong.\n   Batch 50/225 xong.\n   Batch 60/225 xong.\n   Batch 70/225 xong.\n   Batch 80/225 xong.\n   Batch 90/225 xong.\n   Batch 100/225 xong.\n   Batch 110/225 xong.\n   Batch 120/225 xong.\n   Batch 130/225 xong.\n   Batch 140/225 xong.\n   Batch 150/225 xong.\n   Batch 160/225 xong.\n   Batch 170/225 xong.\n   Batch 180/225 xong.\n   Batch 190/225 xong.\n   Batch 200/225 xong.\n   Batch 210/225 xong.\n   Batch 220/225 xong.\n✅ HOÀN TẤT! File kết quả: submission.tsv\n","output_type":"stream"}],"execution_count":30}]}